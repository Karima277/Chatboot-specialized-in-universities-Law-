{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf0e563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:05:47.229203Z",
     "iopub.status.busy": "2024-11-06T16:05:47.228113Z",
     "iopub.status.idle": "2024-11-06T16:06:09.342790Z",
     "shell.execute_reply": "2024-11-06T16:06:09.341700Z"
    },
    "papermill": {
     "duration": 22.129942,
     "end_time": "2024-11-06T16:06:09.345273",
     "exception": false,
     "start_time": "2024-11-06T16:05:47.215331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zenml[all]\r\n",
      "  Downloading zenml-0.68.1-py3-none-any.whl.metadata (21 kB)\r\n",
      "\u001b[33mWARNING: zenml 0.68.1 does not provide the extra 'all'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting alembic<1.9.0,>=1.8.1 (from zenml[all])\r\n",
      "  Downloading alembic-1.8.1-py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting bcrypt==4.0.1 (from zenml[all])\r\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl.metadata (9.0 kB)\r\n",
      "Collecting click<8.1.4,>=8.0.1 (from zenml[all])\r\n",
      "  Downloading click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting cloudpickle<3,>=2.0.0 (from zenml[all])\r\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: distro<2.0.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (1.9.0)\r\n",
      "Requirement already satisfied: docker<7.2.0,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (7.1.0)\r\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.18 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (3.1.43)\r\n",
      "Collecting packaging>=24.1 (from zenml[all])\r\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting passlib<1.8.0,>=1.7.4 (from passlib[bcrypt]<1.8.0,>=1.7.4->zenml[all])\r\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (5.9.3)\r\n",
      "Collecting pydantic<2.9,>=2.8 (from zenml[all])\r\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pydantic-settings (from zenml[all])\r\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting pymysql<1.2.0,>=1.1.1 (from zenml[all])\r\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (6.0.2)\r\n",
      "Requirement already satisfied: rich>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from rich[jupyter]>=12.0.0->zenml[all]) (13.7.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (70.0.0)\r\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from zenml[all]) (2.0.30)\r\n",
      "Collecting sqlalchemy_utils (from zenml[all])\r\n",
      "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Collecting sqlmodel==0.0.18 (from zenml[all])\r\n",
      "  Downloading sqlmodel-0.0.18-py3-none-any.whl.metadata (9.8 kB)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic<1.9.0,>=1.8.1->zenml[all]) (1.3.5)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<7.2.0,>=7.1.0->zenml[all]) (2.32.3)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<7.2.0,>=7.1.0->zenml[all]) (1.26.18)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4.0.0,>=3.1.18->zenml[all]) (4.0.11)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.9,>=2.8->zenml[all]) (0.7.0)\r\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<2.9,>=2.8->zenml[all])\r\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.9,>=2.8->zenml[all]) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.1->zenml[all]) (1.16.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[all]) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[all]) (2.18.0)\r\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.1 in /opt/conda/lib/python3.10/site-packages (from rich[jupyter]>=12.0.0->zenml[all]) (7.7.1)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3.0.0,>=2.0.0->zenml[all]) (3.0.3)\r\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings->zenml[all]) (1.0.1)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.18->zenml[all]) (5.0.1)\r\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (6.29.4)\r\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.2.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (5.14.3)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (3.6.9)\r\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (8.21.0)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (3.0.11)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[all]) (0.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[all]) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[all]) (3.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[all]) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic<1.9.0,>=1.8.1->zenml[all]) (2.1.5)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.2.2)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.8.1)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (7.4.9)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (5.7.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.1.7)\r\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (26.0.3)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (6.4.1)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.19.1)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (3.0.47)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.6.2)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (4.9.0)\r\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (6.5.7)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.8.4)\r\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.4)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (3.11.0)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (3.1.4)\r\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (23.1.0)\r\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (5.10.4)\r\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (6.4.5)\r\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.8.3)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.18.1)\r\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.20.0)\r\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.1.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.2.2)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.2.4)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.8.4)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.3.0)\r\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (6.1.0)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.5.0)\r\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.6.0)\r\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.7.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (4.12.3)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.5.13)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.19.1)\r\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (4.22.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (21.2.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.18.1)\r\n",
      "Requirement already satisfied: jupyter-server<3,>=1.8 in /opt/conda/lib/python3.10/site-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.12.5)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.16.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.5)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.5.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.22)\r\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (4.4.0)\r\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.10.0)\r\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.5.3)\r\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (7.7.0)\r\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.8.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.3.1)\r\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.0.7)\r\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.1.4)\r\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (0.1.1)\r\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.5.1)\r\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (20.11.0)\r\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.4)\r\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.3.0)\r\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (24.6.0)\r\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (1.3.0)\r\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[all]) (2.9.0.20240316)\r\n",
      "Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sqlmodel-0.0.18-py3-none-any.whl (26 kB)\r\n",
      "Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading click-8.1.3-py3-none-any.whl (96 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\r\n",
      "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading zenml-0.68.1-py3-none-any.whl (4.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: passlib, pymysql, pydantic-core, packaging, cloudpickle, click, bcrypt, sqlalchemy_utils, pydantic, alembic, sqlmodel, pydantic-settings, zenml\r\n",
      "  Attempting uninstall: pydantic-core\r\n",
      "    Found existing installation: pydantic_core 2.23.4\r\n",
      "    Uninstalling pydantic_core-2.23.4:\r\n",
      "      Successfully uninstalled pydantic_core-2.23.4\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: cloudpickle\r\n",
      "    Found existing installation: cloudpickle 3.0.0\r\n",
      "    Uninstalling cloudpickle-3.0.0:\r\n",
      "      Successfully uninstalled cloudpickle-3.0.0\r\n",
      "  Attempting uninstall: click\r\n",
      "    Found existing installation: click 8.1.7\r\n",
      "    Uninstalling click-8.1.7:\r\n",
      "      Successfully uninstalled click-8.1.7\r\n",
      "  Attempting uninstall: pydantic\r\n",
      "    Found existing installation: pydantic 2.9.2\r\n",
      "    Uninstalling pydantic-2.9.2:\r\n",
      "      Successfully uninstalled pydantic-2.9.2\r\n",
      "  Attempting uninstall: alembic\r\n",
      "    Found existing installation: alembic 1.13.3\r\n",
      "    Uninstalling alembic-1.13.3:\r\n",
      "      Successfully uninstalled alembic-1.13.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\r\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "dask 2024.9.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\r\n",
      "jupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed alembic-1.8.1 bcrypt-4.0.1 click-8.1.3 cloudpickle-2.2.1 packaging-24.1 passlib-1.7.4 pydantic-2.8.2 pydantic-core-2.20.1 pydantic-settings-2.6.1 pymysql-1.1.1 sqlalchemy_utils-0.41.2 sqlmodel-0.0.18 zenml-0.68.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install zenml[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95227c3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:06:09.377980Z",
     "iopub.status.busy": "2024-11-06T16:06:09.377109Z",
     "iopub.status.idle": "2024-11-06T16:06:30.390774Z",
     "shell.execute_reply": "2024-11-06T16:06:30.389754Z"
    },
    "papermill": {
     "duration": 21.032652,
     "end_time": "2024-11-06T16:06:30.393345",
     "exception": false,
     "start_time": "2024-11-06T16:06:09.360693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[1;35mInitializing the ZenML global configuration version to 0.68.1\u001b[0m\r\n",
      "\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[1;35mCreating database tables\u001b[0m\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[1;35mCreating default workspace 'default' ...\u001b[0m\r\n",
      "\u001b[1;35mCreating default stack in workspace default...\u001b[0m\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[1;35mSetting the global active workspace to 'default'.\u001b[0m\r\n",
      "\u001b[33mSetting the global active stack to default.\u001b[0m\r\n",
      "\u001b[1;35mSetting the repo active workspace to 'default'.\u001b[0m\r\n",
      "\u001b[33mSetting the repo active stack to default.\u001b[0m\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\u001b[1;35mReloading configuration file /kaggle/working/.zen/config.yaml\u001b[0m\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mZenML repository initialized at \u001b[0m\u001b[2;35m/kaggle/\u001b[0m\u001b[2;95mworking.\u001b[0m\r\n",
      "\u001b[2;32m⠴\u001b[0m\u001b[2;36m Initializing ZenML repository at /kaggle/working.\u001b[0m\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /kaggle/working.\r\n",
      "\r\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mThe local active stack was initialized to \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m. This local configuration \u001b[0m\r\n",
      "\u001b[2;36mwill only take effect when you're running ZenML from the initialized repository \u001b[0m\r\n",
      "\u001b[2;36mroot, or from a subdirectory. For more information on repositories and \u001b[0m\r\n",
      "\u001b[2;36mconfigurations, please visit \u001b[0m\r\n",
      "\u001b[2;4;94mhttps://docs.zenml.io/user-guide/starter-guide/understand-stacks.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!zenml init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a3f14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:06:30.439606Z",
     "iopub.status.busy": "2024-11-06T16:06:30.439179Z",
     "iopub.status.idle": "2024-11-06T16:06:42.586919Z",
     "shell.execute_reply": "2024-11-06T16:06:42.585648Z"
    },
    "papermill": {
     "duration": 12.173639,
     "end_time": "2024-11-06T16:06:42.589795",
     "exception": false,
     "start_time": "2024-11-06T16:06:30.416156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\r\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\r\n",
      "Successfully installed PyPDF2-3.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d86ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:06:42.635653Z",
     "iopub.status.busy": "2024-11-06T16:06:42.635241Z",
     "iopub.status.idle": "2024-11-06T16:06:57.747471Z",
     "shell.execute_reply": "2024-11-06T16:06:57.746142Z"
    },
    "papermill": {
     "duration": 15.138339,
     "end_time": "2024-11-06T16:06:57.750175",
     "exception": false,
     "start_time": "2024-11-06T16:06:42.611836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain)\r\n",
      "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.1.140-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.2)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\r\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\r\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.4)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\r\n",
      "Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\r\n",
      "Downloading langsmith-0.1.140-py3-none-any.whl (304 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\r\n",
      "  Attempting uninstall: requests-toolbelt\r\n",
      "    Found existing installation: requests-toolbelt 0.10.1\r\n",
      "    Uninstalling requests-toolbelt-0.10.1:\r\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed langchain-0.3.7 langchain-core-0.3.15 langchain-text-splitters-0.3.2 langsmith-0.1.140 requests-toolbelt-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a5ba01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:06:57.798619Z",
     "iopub.status.busy": "2024-11-06T16:06:57.798219Z",
     "iopub.status.idle": "2024-11-06T16:07:09.917797Z",
     "shell.execute_reply": "2024-11-06T16:07:09.916688Z"
    },
    "papermill": {
     "duration": 12.146668,
     "end_time": "2024-11-06T16:07:09.920288",
     "exception": false,
     "start_time": "2024-11-06T16:06:57.773620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\r\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0+cpu)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\r\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\r\n",
      "Successfully installed sentence_transformers-3.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0540ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:07:09.969306Z",
     "iopub.status.busy": "2024-11-06T16:07:09.968844Z",
     "iopub.status.idle": "2024-11-06T16:07:28.366802Z",
     "shell.execute_reply": "2024-11-06T16:07:28.365660Z"
    },
    "papermill": {
     "duration": 18.425461,
     "end_time": "2024-11-06T16:07:28.369299",
     "exception": false,
     "start_time": "2024-11-06T16:07:09.943838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mDeleted local ZenML config from \u001b[0m\u001b[2;35m/kaggle/working/\u001b[0m\u001b[2;95m.zen.\u001b[0m\r\n",
      "\u001b[33mDeleting \u001b[0m\u001b[32m'local_stores'\u001b[0m\u001b[33m directory from global config.\u001b[0m\r\n",
      "\u001b[2;36mDeleted global ZenML config from \u001b[0m\u001b[2;35m/root/.config/\u001b[0m\u001b[2;95mzenml.\u001b[0m\r\n",
      "\u001b[1;35mCreating database tables\u001b[0m\r\n",
      "\u001b[1;35mCreating default workspace 'default' ...\u001b[0m\r\n",
      "\u001b[1;35mCreating default stack in workspace default...\u001b[0m\r\n",
      "\u001b[1;35mSetting the global active workspace to 'default'.\u001b[0m\r\n",
      "\u001b[33mSetting the global active stack to default.\u001b[0m\r\n",
      "\u001b[2;36mReinitialized ZenML global config at \u001b[0m\u001b[2;35m/kaggle/\u001b[0m\u001b[2;95mworking.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!zenml clean -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e072e6be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:07:28.419220Z",
     "iopub.status.busy": "2024-11-06T16:07:28.418770Z",
     "iopub.status.idle": "2024-11-06T16:07:39.233340Z",
     "shell.execute_reply": "2024-11-06T16:07:39.232086Z"
    },
    "papermill": {
     "duration": 10.842346,
     "end_time": "2024-11-06T16:07:39.235708",
     "exception": false,
     "start_time": "2024-11-06T16:07:28.393362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: zenml clean [OPTIONS]\r\n",
      "Try 'zenml clean --help' for help.\r\n",
      "\r\n",
      "Error: No such option: --steps (Possible options: --help, --yes)\r\n"
     ]
    }
   ],
   "source": [
    "!zenml clean --steps extract_pdf_text preprocess_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9658257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:07:39.286579Z",
     "iopub.status.busy": "2024-11-06T16:07:39.285776Z",
     "iopub.status.idle": "2024-11-06T16:07:51.011585Z",
     "shell.execute_reply": "2024-11-06T16:07:51.010226Z"
    },
    "papermill": {
     "duration": 11.75424,
     "end_time": "2024-11-06T16:07:51.014237",
     "exception": false,
     "start_time": "2024-11-06T16:07:39.259997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee0dbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:07:51.065759Z",
     "iopub.status.busy": "2024-11-06T16:07:51.065337Z",
     "iopub.status.idle": "2024-11-06T16:08:32.045566Z",
     "shell.execute_reply": "2024-11-06T16:08:32.044090Z"
    },
    "papermill": {
     "duration": 41.009132,
     "end_time": "2024-11-06T16:08:32.048010",
     "exception": true,
     "start_time": "2024-11-06T16:07:51.038878",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mNumExpr defaulting to 4 threads.\u001b[0m\n",
      "\u001b[1;35mPyTorch version 2.4.0+cpu available.\u001b[0m\n",
      "\u001b[1;35mPolars version 1.9.0 available.\u001b[0m\n",
      "\u001b[1;35mTensorFlow version 2.16.1 available.\u001b[0m\n",
      "\u001b[1;35mJAX version 0.4.33 available.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mrag_pipeline\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">127</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> == <span style=\"color: #808000; text-decoration-color: #808000\">\"__main__\"</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Run the pipeline</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>127 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>rag_pipeline(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>pdf_path=<span style=\"color: #808000; text-decoration-color: #808000\">\"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseigneme</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">130 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pipeline_definition.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1346</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1343 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.entrypoint(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare(*args, **kwargs)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1346 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run(**<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_args)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1347 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1348 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_entrypoint</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args: Any, **kwargs: Any) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1349 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Calls the pipeline entrypoint function with the given arguments.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pipeline_definition.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">610</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 607 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Initiating a new run for the pipeline: `{</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name<span style=\"color: #808000; text-decoration-color: #808000\">}`.\"</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 608 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 609 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> track_handler(AnalyticsEvent.RUN_PIPELINE) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> analytics_handler:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 610 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>deployment, schedule, build = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compile(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 611 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>config_path=config_path,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 612 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>run_name=run_name,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 613 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>enable_cache=enable_cache,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pipeline_definition.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1006</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_compile</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1003 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1004 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>deployment = Compiler().compile(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1005 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>pipeline=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1006 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>stack=Client().active_stack,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1007 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>run_configuration=run_config,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1008 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1009 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>deployment = env_utils.substitute_env_variable_placeholders(deployment)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1453</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">active_stack</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1450 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1451 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">zenml.stack.stack</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Stack                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1452 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1453 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Stack.from_model(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.active_stack_model)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1454 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1455 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@property</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">active_stack_model</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; StackResponse:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/stack/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">stack.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">166</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_model</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hydrate=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>166 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>stack_components = {                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model.type: StackComponent.from_model(model)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> model <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> component_models                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>}                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/stack/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">stack.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">167</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;dictcomp&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>stack_components = {                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>167 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model.type: StackComponent.from_model(model)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> model <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> component_models                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>}                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>stack = Stack.from_components(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/stack/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">stack_component.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">411</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_model</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">408 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">409 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">zenml.stack</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Flavor                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">410 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>411 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>flavor = Flavor.from_model(flavor_model)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">412 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">ModuleNotFoundError</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ImportError</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">NotImplementedError</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">413 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ImportError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">414 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Couldn't import flavor {</span>flavor_model.name<span style=\"color: #808000; text-decoration-color: #808000\">}: {</span>err<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/stack/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">flavor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">131</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_model</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Returns:</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">129 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">The loaded flavor.</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">130 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>131 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>flavor = source_utils.load(flavor_model.source)()                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cast(Flavor, flavor)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to_model</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">source_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">122</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> source.type <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> {SourceType.USER, SourceType.UNKNOWN}:                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Unknown source might also refer to a user file, include source</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># root in python path just to be sure</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>122 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>import_root = get_source_root()                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module = _load_module(module_name=source.module, import_root=import_root)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/zenml/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">source_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">293</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_source_root</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(main_module, <span style=\"color: #808000; text-decoration-color: #808000\">\"__file__\"</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> main_module.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__file__</span>:                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>293 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Unable to determine source root because the main module does not \"</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">295 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"have an associated file. This could be because you're running in \"</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"an interactive Python environment. If you are trying to run from \"</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Unable to determine source root because the main module does not have an associated file. This could \n",
       "be because you're running in an interactive Python environment. If you are trying to run from within a Jupyter \n",
       "notebook, please run `zenml init` from the root where your notebook is located and restart your notebook server.   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m127\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Run the pipeline\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m127 \u001b[2m│   \u001b[0mrag_pipeline(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2m│   │   \u001b[0mpdf_path=\u001b[33m\"\u001b[0m\u001b[33m/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseigneme\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m129 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/pipelines/\u001b[0m\u001b[1;33mpipeline_definition.py\u001b[0m:\u001b[94m1346\u001b[0m in \u001b[92m__call__\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1343 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.entrypoint(*args, **kwargs)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1344 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1345 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.prepare(*args, **kwargs)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1346 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._run(**\u001b[96mself\u001b[0m._run_args)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1347 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1348 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_entrypoint\u001b[0m(\u001b[96mself\u001b[0m, *args: Any, **kwargs: Any) -> \u001b[94mNone\u001b[0m:                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1349 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Calls the pipeline entrypoint function with the given arguments.\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/pipelines/\u001b[0m\u001b[1;33mpipeline_definition.py\u001b[0m:\u001b[94m610\u001b[0m in \u001b[92m_run\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 607 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mInitiating a new run for the pipeline: `\u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.name\u001b[33m}\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m\"\u001b[0m)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 608 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 609 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m track_handler(AnalyticsEvent.RUN_PIPELINE) \u001b[94mas\u001b[0m analytics_handler:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 610 \u001b[2m│   │   │   \u001b[0mdeployment, schedule, build = \u001b[96mself\u001b[0m._compile(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 611 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_path=config_path,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 612 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_name=run_name,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 613 \u001b[0m\u001b[2m│   │   │   │   \u001b[0menable_cache=enable_cache,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/pipelines/\u001b[0m\u001b[1;33mpipeline_definition.py\u001b[0m:\u001b[94m1006\u001b[0m in \u001b[92m_compile\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1004 \u001b[0m\u001b[2m│   │   \u001b[0mdeployment = Compiler().compile(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1005 \u001b[0m\u001b[2m│   │   │   \u001b[0mpipeline=\u001b[96mself\u001b[0m,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1006 \u001b[2m│   │   │   \u001b[0mstack=Client().active_stack,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1007 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_configuration=run_config,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1008 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1009 \u001b[0m\u001b[2m│   │   \u001b[0mdeployment = env_utils.substitute_env_variable_placeholders(deployment)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1453\u001b[0m in \u001b[92mactive_stack\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1450 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1451 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mzenml\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mstack\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mstack\u001b[0m \u001b[94mimport\u001b[0m Stack                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1452 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1453 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m Stack.from_model(\u001b[96mself\u001b[0m.active_stack_model)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1454 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1455 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@property\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mactive_stack_model\u001b[0m(\u001b[96mself\u001b[0m) -> StackResponse:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/stack/\u001b[0m\u001b[1;33mstack.py\u001b[0m:\u001b[94m166\u001b[0m in \u001b[92mfrom_model\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   \u001b[0mhydrate=\u001b[94mTrue\u001b[0m,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m166 \u001b[2m│   │   \u001b[0mstack_components = {                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel.type: StackComponent.from_model(model)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m model \u001b[95min\u001b[0m component_models                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/stack/\u001b[0m\u001b[1;33mstack.py\u001b[0m:\u001b[94m167\u001b[0m in \u001b[92m<dictcomp>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0mstack_components = {                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m167 \u001b[2m│   │   │   \u001b[0mmodel.type: StackComponent.from_model(model)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m model \u001b[95min\u001b[0m component_models                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   \u001b[0mstack = Stack.from_components(                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/stack/\u001b[0m\u001b[1;33mstack_component.py\u001b[0m:\u001b[94m411\u001b[0m in \u001b[92mfrom_model\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m408 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m409 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mzenml\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mstack\u001b[0m \u001b[94mimport\u001b[0m Flavor                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m410 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m411 \u001b[2m│   │   │   \u001b[0mflavor = Flavor.from_model(flavor_model)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m412 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mModuleNotFoundError\u001b[0m, \u001b[96mImportError\u001b[0m, \u001b[96mNotImplementedError\u001b[0m) \u001b[94mas\u001b[0m err:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m413 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mImportError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m414 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt import flavor \u001b[0m\u001b[33m{\u001b[0mflavor_model.name\u001b[33m}\u001b[0m\u001b[33m: \u001b[0m\u001b[33m{\u001b[0merr\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/stack/\u001b[0m\u001b[1;33mflavor.py\u001b[0m:\u001b[94m131\u001b[0m in \u001b[92mfrom_model\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mReturns:\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m129 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mThe loaded flavor.\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m131 \u001b[2m│   │   \u001b[0mflavor = source_utils.load(flavor_model.source)()                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(Flavor, flavor)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mto_model\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/utils/\u001b[0m\u001b[1;33msource_utils.py\u001b[0m:\u001b[94m122\u001b[0m in \u001b[92mload\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m source.type \u001b[95min\u001b[0m {SourceType.USER, SourceType.UNKNOWN}:                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Unknown source might also refer to a user file, include source\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# root in python path just to be sure\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m122 \u001b[2m│   │   \u001b[0mimport_root = get_source_root()                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   \u001b[0mmodule = _load_module(module_name=source.module, import_root=import_root)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/zenml/utils/\u001b[0m\u001b[1;33msource_utils.py\u001b[0m:\u001b[94m293\u001b[0m in \u001b[92mget_source_root\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mhasattr\u001b[0m(main_module, \u001b[33m\"\u001b[0m\u001b[33m__file__\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mor\u001b[0m \u001b[95mnot\u001b[0m main_module.\u001b[91m__file__\u001b[0m:                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m293 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnable to determine source root because the main module does not \u001b[0m\u001b[33m\"\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mhave an associated file. This could be because you\u001b[0m\u001b[33m'\u001b[0m\u001b[33mre running in \u001b[0m\u001b[33m\"\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33man interactive Python environment. If you are trying to run from \u001b[0m\u001b[33m\"\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mUnable to determine source root because the main module does not have an associated file. This could \n",
       "be because you're running in an interactive Python environment. If you are trying to run from within a Jupyter \n",
       "notebook, please run `zenml init` from the root where your notebook is located and restart your notebook server.   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def create_semantic_search_index(text_chunks: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = embedding_model.encode(text_chunks)\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "@step\n",
    "def prepare_training_data(text_chunks: List[str], model_name: str = 'camembert/camembert-base') -> Dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    questions = [f\"Quel est le contenu de cet extrait de la loi 00.01 ?\" for _ in text_chunks]\n",
    "    encodings = tokenizer(questions, text_chunks, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    training_data = {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'start_positions': [0] * len(text_chunks),\n",
    "        'end_positions': [len(chunk.split()) for chunk in text_chunks]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(training_data)\n",
    "    return dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "@step\n",
    "def fine_tune_model(train_test_split: Dict, model_name: str = 'camembert/camembert-base') -> str:\n",
    "    train_dataset = train_test_split['train']\n",
    "    val_dataset = train_test_split['test']\n",
    "    \n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_loi_00_01',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    model_path = './loi_00_01_model'\n",
    "    trainer.save_model(model_path)\n",
    "    return model_path\n",
    "\n",
    "@step\n",
    "def inference_step(\n",
    "    question: str,\n",
    "    embeddings: np.ndarray,\n",
    "    text_chunks: List[str],\n",
    "    model_path: str\n",
    ") -> str:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"camembert/camembert-base\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    similarities = pairwise.cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "    context_string = \"\\n\".join([text_chunks[i] for i in top_indices])\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context_string,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits)\n",
    "    answer_tokens = inputs['input_ids'][0][start_index:end_index+1]\n",
    "    return tokenizer.decode(answer_tokens)\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(pdf_path: str):\n",
    "    # Execute pipeline steps sequentially\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    embeddings, indexed_chunks = create_semantic_search_index(text_chunks)\n",
    "    train_test_split = prepare_training_data(text_chunks)\n",
    "    model_path = fine_tune_model(train_test_split)\n",
    "    \n",
    "    # Define test question\n",
    "    question = \"Quelles sont les missions de l'enseignement supérieur selon la loi 00.01 ?\"\n",
    "    answer = inference_step(question, embeddings, indexed_chunks, model_path)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    rag_pipeline(\n",
    "        pdf_path=\"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923139ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T16:10:25.401685Z",
     "iopub.status.busy": "2024-11-03T16:10:25.400776Z",
     "iopub.status.idle": "2024-11-03T16:10:37.828112Z",
     "shell.execute_reply": "2024-11-03T16:10:37.826433Z",
     "shell.execute_reply.started": "2024-11-03T16:10:25.401628Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71bccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T16:13:16.008771Z",
     "iopub.status.busy": "2024-11-03T16:13:16.008285Z",
     "iopub.status.idle": "2024-11-03T16:14:50.803653Z",
     "shell.execute_reply": "2024-11-03T16:14:50.802300Z",
     "shell.execute_reply.started": "2024-11-03T16:13:16.008728Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c690dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T21:51:36.274211Z",
     "iopub.status.busy": "2024-11-03T21:51:36.273622Z",
     "iopub.status.idle": "2024-11-03T21:52:14.097292Z",
     "shell.execute_reply": "2024-11-03T21:52:14.095809Z",
     "shell.execute_reply.started": "2024-11-03T21:51:36.274163Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def create_embeddings_index(\n",
    "    text_chunks: List[str],\n",
    "    #all-MiniLM-L6-v2\n",
    "    #paraphrase-multilingual-mpnet-base-v2\n",
    "    model_name: str = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Crée les embeddings pour chaque chunk de texte.\"\"\"\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    embeddings = embedding_model.encode(text_chunks)\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    embeddings: np.ndarray,\n",
    "    text_chunks: List[str],\n",
    "    top_k: int = 5\n",
    ") -> str:\n",
    "    \"\"\"Récupère les chunks les plus pertinents pour la question.\"\"\"\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    \n",
    "    # Calcul des similarités\n",
    "    similarities = pairwise.cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Concaténation des chunks les plus pertinents\n",
    "    return \"\\n\".join([text_chunks[i] for i in top_indices])\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse à partir du contexte récupéré.\"\"\"\n",
    "    qa_pipeline = hf_pipeline(\n",
    "        \"question-answering\",\n",
    "        #model=\"camembert/camembert-base\",\n",
    "        #camembert/camembert-base\n",
    "        model=\"etalab-ia/camembert-base-squadFR-fquad-piaf\",\n",
    "        tokenizer=\"etalab-ia/camembert-base-squadFR-fquad-piaf\"\n",
    "        #tokenizer=\"camembert/camembert-base\"\n",
    "    )\n",
    "    \n",
    "    result = qa_pipeline(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    # Extraction et préparation du texte\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    \n",
    "    # Création des embeddings\n",
    "    embeddings, indexed_chunks = create_embeddings_index(text_chunks)\n",
    "    \n",
    "    # Récupération des chunks pertinents\n",
    "    relevant_context = retrieve_relevant_chunks(question, embeddings, indexed_chunks)\n",
    "    \n",
    "    # Génération de la réponse\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute pipeline steps directly for debugging\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"Donne moi un loi parmi les lois ?\"\n",
    "    \n",
    "    try:\n",
    "        # Extract text from PDF\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        # Modify the preprocessing to get larger chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1500,  # Increased from 500\n",
    "            chunk_overlap=300,  # Increased from 50\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "        text_chunks = text_splitter.split_text(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings, indexed_chunks = create_embeddings_index.entrypoint(text_chunks)\n",
    "        print(\"Embeddings created successfully\")\n",
    "        \n",
    "        # Get more relevant chunks\n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            embeddings=embeddings,\n",
    "            text_chunks=indexed_chunks,\n",
    "            top_k=10\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        # Generate answer with modified parameters\n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"Réponse:\", final_answer)\n",
    "        \n",
    "        # Print the context for debugging\n",
    "        #print(\"\\nContexte utilisé:\")\n",
    "        #print(relevant_context)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434387c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T19:24:50.643198Z",
     "iopub.status.busy": "2024-11-03T19:24:50.641959Z",
     "iopub.status.idle": "2024-11-03T19:24:50.842503Z",
     "shell.execute_reply": "2024-11-03T19:24:50.841017Z",
     "shell.execute_reply.started": "2024-11-03T19:24:50.643148Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af2916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T21:08:06.569703Z",
     "iopub.status.busy": "2024-11-03T21:08:06.568231Z",
     "iopub.status.idle": "2024-11-03T21:18:12.018359Z",
     "shell.execute_reply": "2024-11-03T21:18:12.015906Z",
     "shell.execute_reply.started": "2024-11-03T21:08:06.569639Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,  # Increased chunk size for more context\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def create_embeddings_index(\n",
    "    text_chunks: List[str],\n",
    "    model_name: str = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Crée les embeddings pour chaque chunk de texte.\"\"\"\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    embeddings = embedding_model.encode(text_chunks)\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    embeddings: np.ndarray,\n",
    "    text_chunks: List[str],\n",
    "    top_k: int = 8  # Increased from 5 to get more context\n",
    ") -> str:\n",
    "    \"\"\"Récupère les chunks les plus pertinents pour la question.\"\"\"\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    \n",
    "    # Calcul des similarités\n",
    "    similarities = pairwise.cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Format the context with clear separation and relevance scores\n",
    "    formatted_chunks = []\n",
    "    for idx in top_indices:\n",
    "        score = similarities[idx]\n",
    "        chunk = text_chunks[idx].strip()\n",
    "        formatted_chunks.append(f\"{chunk}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse détaillée à partir du contexte récupéré.\"\"\"\n",
    "    # Initialize the model pipeline\n",
    "    generator = hf_pipeline(\n",
    "        \"text-generation\",\n",
    "        # Option 1: BLOOMZ (multilingual, good for French)\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        tokenizer=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        # Option 2: Mistral (if you prefer)\n",
    "        # model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        # tokenizer=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Craft a detailed prompt in French\n",
    "    prompt = f\"\"\"Tu es un assistant spécialisé dans l'analyse de documents juridiques. Sur la base du contexte fourni, réponds de manière détaillée et structurée à la question posée.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Contexte du document:\n",
    "{context[:3500]}\n",
    "\n",
    "Instructions:\n",
    "- Extrais les informations pertinentes du contexte\n",
    "- Structure ta réponse en points clairs\n",
    "- Cite les passages importants du texte\n",
    "- Organise les informations de manière logique\n",
    "\n",
    "Réponse structurée:\"\"\"\n",
    "\n",
    "    # Generate response with optimized parameters\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=200,\n",
    "        min_length=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.4, \n",
    "        top_p=0.90,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        eos_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Clean up the response\n",
    "    generated_text = response[0]['generated_text']\n",
    "    answer_start = generated_text.find(\"Réponse structurée:\") + len(\"Réponse structurée:\")\n",
    "    answer = generated_text[answer_start:].strip()\n",
    "    \n",
    "    # Post-process to ensure proper formatting\n",
    "    if not any(answer.startswith(prefix) for prefix in [\"Selon\", \"D'après\", \"Sur base\"]):\n",
    "        answer = \"Selon l'analyse du document, \" + answer\n",
    "    \n",
    "    return answer\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    embeddings, indexed_chunks = create_embeddings_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, embeddings, indexed_chunks)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"Quelles sont les missions de l'enseignement supérieur selon la loi 00.01 ?\"\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        embeddings, indexed_chunks = create_embeddings_index.entrypoint(text_chunks)\n",
    "        print(\"Embeddings created successfully\")\n",
    "        \n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            embeddings=embeddings,\n",
    "            text_chunks=indexed_chunks,\n",
    "            top_k=8  # Increased for more context\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"\\nRéponse:\")\n",
    "        print(final_answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cf68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T22:15:58.860016Z",
     "iopub.status.busy": "2024-11-03T22:15:58.859446Z",
     "iopub.status.idle": "2024-11-03T22:16:15.415827Z",
     "shell.execute_reply": "2024-11-03T22:16:15.414192Z",
     "shell.execute_reply.started": "2024-11-03T22:15:58.859955Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260609e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T22:22:43.514290Z",
     "iopub.status.busy": "2024-11-03T22:22:43.513596Z",
     "iopub.status.idle": "2024-11-03T22:22:55.875303Z",
     "shell.execute_reply": "2024-11-03T22:22:55.873334Z",
     "shell.execute_reply.started": "2024-11-03T22:22:43.514230Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27739a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# GEMINI for Answers x SentenceTransformers  for embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b3088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T23:11:50.285114Z",
     "iopub.status.busy": "2024-11-03T23:11:50.284274Z",
     "iopub.status.idle": "2024-11-03T23:12:18.042309Z",
     "shell.execute_reply": "2024-11-03T23:12:18.041147Z",
     "shell.execute_reply.started": "2024-11-03T23:11:50.285067Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "import google.generativeai as palm\n",
    "\n",
    "palm.configure(api_key=\"AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM\")\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement optimisé.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,  # Augmenté pour plus de contexte\n",
    "        chunk_overlap=400,  # Augmenté pour meilleure cohérence\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,  # Increased chunk size for more context\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def create_embeddings_index(\n",
    "    text_chunks: List[str],\n",
    "    model_name: str = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Crée les embeddings pour chaque chunk de texte.\"\"\"\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    embeddings = embedding_model.encode(text_chunks)\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    embeddings: np.ndarray,\n",
    "    text_chunks: List[str],\n",
    "    top_k: int = 12  # Augmenté pour avoir plus de contexte\n",
    ") -> str:\n",
    "    \"\"\"Récupère les chunks les plus pertinents pour la question.\"\"\"\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    \n",
    "    # Calcul des similarités\n",
    "    similarities = pairwise.cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Format the context with relevance scores\n",
    "    formatted_chunks = []\n",
    "    for idx in top_indices:\n",
    "        score = similarities[idx]\n",
    "        chunk = text_chunks[idx].strip()\n",
    "        if score > 0.3:  # Seuil de pertinence minimum\n",
    "            formatted_chunks.append(f\"{chunk}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key='AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM')\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse détaillée à partir du contexte récupéré en utilisant Gemini.\"\"\"\n",
    "    \n",
    "    # Configure le modèle Gemini\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    # Création du prompt\n",
    "    prompt = f\"\"\"Tu es un expert juridique spécialisé dans l'analyse des lois sur l'enseignement supérieur.\n",
    "Analyse le texte de loi suivant et réponds à la question de manière détaillée et structurée.\n",
    "\n",
    "Texte de la loi 00.01:\n",
    "{context[:10000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions pour ta réponse:\n",
    "1. Commence par une brève introduction sur le cadre légal\n",
    "2. Liste toutes les missions définies dans la loi\n",
    "3. Pour chaque mission:\n",
    "   - Explique son importance et sa portée\n",
    "   - Cite le passage exact du texte qui la définit\n",
    "4. Organise ta réponse de manière claire avec des puces ou numéros\n",
    "5. Termine par une brève conclusion\n",
    "\n",
    "Format attendu:\n",
    "• Points structurés\n",
    "• Citations exactes entre guillemets\n",
    "• Explications détaillées\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Génération de la réponse\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Extraction et formatage de la réponse\n",
    "        answer = response.text.strip()\n",
    "        \n",
    "        # Vérification et formatage de la réponse\n",
    "        if len(answer.split()) < 50:  # Si la réponse est trop courte\n",
    "            # Deuxième tentative avec un prompt plus direct\n",
    "            prompt_retry = f\"\"\"Analyse et liste les missions de l'enseignement supérieur définies dans ce texte de loi:\n",
    "\n",
    "{context[:8000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Réponds de manière détaillée en citant le texte.\"\"\"\n",
    "            \n",
    "            response = model.generate_content(prompt_retry)\n",
    "            answer = response.text.strip()\n",
    "        \n",
    "        # Ajout d'une introduction si nécessaire\n",
    "        if not any(answer.lower().startswith(prefix) for prefix in \n",
    "                  ['selon', 'conformément', 'daprès', 'la loi']):\n",
    "            answer = \"Selon la loi 00.01, \" + answer[0].lower() + answer[1:]\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Une erreur s'est produite lors de la génération de la réponse: {str(e)}\"\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    embeddings, indexed_chunks = create_embeddings_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, embeddings, indexed_chunks)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"dont moi un loi parmi ces lois ?\"\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        embeddings, indexed_chunks = create_embeddings_index.entrypoint(text_chunks)\n",
    "        print(\"Embeddings created successfully\")\n",
    "        \n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            embeddings=embeddings,\n",
    "            text_chunks=indexed_chunks,\n",
    "            top_k=8  # Increased for more context\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"\\nRéponse:\")\n",
    "        print(final_answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397f34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:24:13.851772Z",
     "iopub.status.busy": "2024-11-06T15:24:13.851313Z",
     "iopub.status.idle": "2024-11-06T15:24:27.468758Z",
     "shell.execute_reply": "2024-11-06T15:24:27.467713Z",
     "shell.execute_reply.started": "2024-11-06T15:24:13.851733Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e7aed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# GEMINI for Answers x COHERE  for embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e8d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T11:25:24.515402Z",
     "iopub.status.busy": "2024-11-04T11:25:24.514947Z",
     "iopub.status.idle": "2024-11-04T11:25:32.585176Z",
     "shell.execute_reply": "2024-11-04T11:25:32.584110Z",
     "shell.execute_reply.started": "2024-11-04T11:25:24.515362Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from sentence_transformers import SenatenceTransformer\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import cohere\n",
    "import google.generativeai as palm\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "palm.configure(api_key=\"AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM\")\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement optimisé.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,  # Augmenté pour plus de contexte\n",
    "        chunk_overlap=400,  # Augmenté pour meilleure cohérence\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "# @step\n",
    "# def preprocess_text(raw_text: str) -> List[str]:\n",
    "#     \"\"\"Découpe le texte en chunks avec chevauchement.\"\"\"\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1500,  # Increased chunk size for more context\n",
    "#         chunk_overlap=300,\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "#     )\n",
    "#     return text_splitter.split_text(raw_text)\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "@step\n",
    "def create_embeddings_index(\n",
    "    text_chunks: List[str]\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Creates embeddings for each text chunk using Cohere's API.\"\"\"\n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Generate embeddings in batches to handle API limits\n",
    "    batch_size = 96  # Cohere's recommended batch size\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch = text_chunks[i:i + batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model='embed-multilingual-v3.0',  # Using Cohere's multilingual model\n",
    "            input_type='search_document'\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings)\n",
    "    \n",
    "    # Convert to numpy array for consistency with rest of pipeline\n",
    "    embeddings = np.array(all_embeddings)\n",
    "    \n",
    "    return embeddings, text_chunks\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    embeddings: np.ndarray,\n",
    "    text_chunks: List[str],\n",
    "    top_k: int = 12\n",
    ") -> str:\n",
    "    \"\"\"Retrieves the most relevant chunks for the question using Cohere embeddings.\"\"\"\n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Get embedding for the question\n",
    "    query_response = co.embed(\n",
    "        texts=[question],\n",
    "        model='embed-multilingual-v3.0',\n",
    "        input_type='search_query'\n",
    "    )\n",
    "    query_embedding = np.array(query_response.embeddings)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = np.dot(embeddings, query_embedding.T).flatten()\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Format the context with relevance scores\n",
    "    formatted_chunks = []\n",
    "    for idx in top_indices:\n",
    "        score = similarities[idx]\n",
    "        chunk = text_chunks[idx].strip()\n",
    "        if score > 0.3:  # Minimum relevance threshold\n",
    "            formatted_chunks.append(f\"{chunk}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key='AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM')\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse détaillée à partir du contexte récupéré en utilisant Gemini.\"\"\"\n",
    "    \n",
    "    # Configure le modèle Gemini\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    # Création du prompt amélioré\n",
    "    prompt = f\"\"\"Agis comme un expert juridique spécialisé dans l'analyse des lois sur l'enseignement supérieur au Maroc.\n",
    "\n",
    "Contexte du texte de loi:\n",
    "{context[:10000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions précises pour ta réponse:\n",
    "1. Commence par une introduction citant la loi concernée\n",
    "2. Pour chaque mission mentionnée dans la loi:\n",
    "   - Cite le texte exact entre guillemets\n",
    "   - Explique brièvement la portée de cette mission\n",
    "3. Structure ta réponse avec des tirets ou des puces\n",
    "4. Organise les missions par ordre d'importance\n",
    "5. Ajoute une courte conclusion sur l'importance de ces missions\n",
    "\n",
    "FORMAT ATTENDU:\n",
    "- Introduction\n",
    "- Liste détaillée des missions avec citations\n",
    "- Explication pour chaque mission\n",
    "- Conclusion brève\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Configuration des paramètres de génération\n",
    "        generation_config = {\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.8,\n",
    "            'top_k': 40,\n",
    "            'max_output_tokens': 2048,\n",
    "        }\n",
    "\n",
    "        safety_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'text'):\n",
    "            answer = response.text.strip()\n",
    "        else:\n",
    "            answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "\n",
    "        if len(answer.split()) < 100:  # Seuil augmenté pour assurer une réponse plus complète\n",
    "            prompt_retry = f\"\"\"Analyse et liste en détail toutes les missions des universités définies dans ce texte de loi:\n",
    "\n",
    "{context[:8000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Inclus des citations exactes du texte et explique chaque mission.\"\"\"\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                prompt_retry,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            if hasattr(response, 'text'):\n",
    "                answer = response.text.strip()\n",
    "            else:\n",
    "                answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "        \n",
    "        if not answer:\n",
    "            return \"Désolé, je n'ai pas pu générer une réponse appropriée. Veuillez reformuler votre question.\"\n",
    "        \n",
    "        if not any(answer.lower().startswith(prefix) for prefix in \n",
    "                  ['selon', 'conformément', 'd\\'après', 'la loi']):\n",
    "            answer = \"Selon la loi 01.00 portant sur l'organisation de l'enseignement supérieur, \" + answer[0].lower() + answer[1:]\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        return f\"\"\"Désolé, une erreur est survenue lors de l'analyse du texte de loi. \n",
    "Suggestions:\n",
    "1. Reformulez votre question de manière plus précise\n",
    "2. Vérifiez que votre question porte bien sur le contenu du texte de loi\n",
    "3. Essayez de poser une question plus spécifique sur une mission particulière\n",
    "\n",
    "Détail technique: {error_msg}\"\"\"\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    embeddings, indexed_chunks = create_embeddings_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, embeddings, indexed_chunks)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"quelles sont les mission des universitée ?\"\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        embeddings, indexed_chunks = create_embeddings_index.entrypoint(text_chunks)\n",
    "        print(\"Embeddings created successfully\")\n",
    "        \n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            embeddings=embeddings,\n",
    "            text_chunks=indexed_chunks,\n",
    "            top_k=8  # Increased for more context\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"\\nRéponse:\")\n",
    "        print(final_answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138e814",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# USING Faiss instead of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ea8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:24:32.899104Z",
     "iopub.status.busy": "2024-11-06T15:24:32.897986Z",
     "iopub.status.idle": "2024-11-06T15:24:45.989214Z",
     "shell.execute_reply": "2024-11-06T15:24:45.988171Z",
     "shell.execute_reply.started": "2024-11-06T15:24:32.899044Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340ee6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Main code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8886d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T12:21:17.064953Z",
     "iopub.status.busy": "2024-11-04T12:21:17.064490Z",
     "iopub.status.idle": "2024-11-04T12:21:25.623483Z",
     "shell.execute_reply": "2024-11-04T12:21:25.622106Z",
     "shell.execute_reply.started": "2024-11-04T12:21:17.064908Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import cohere\n",
    "import google.generativeai as palm\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "palm.configure(api_key=\"AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM\")\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement optimisé.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=400,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def create_faiss_index(\n",
    "    text_chunks: List[str]\n",
    ") -> Tuple[np.ndarray, List[str], faiss.IndexFlatIP]:\n",
    "    \"\"\"Creates a FAISS index for the text chunks using Cohere embeddings.\"\"\"\n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 96\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch = text_chunks[i:i + batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model='embed-multilingual-v3.0',\n",
    "            input_type='search_document'\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return embeddings, text_chunks, index\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    index_data: Tuple[np.ndarray, List[str], faiss.IndexFlatIP],\n",
    "    top_k: int = 12\n",
    ") -> str:\n",
    "    \"\"\"Retrieves the most relevant chunks using FAISS.\"\"\"\n",
    "    embeddings, text_chunks, index = index_data\n",
    "    \n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Get embedding for the question\n",
    "    query_response = co.embed(\n",
    "        texts=[question],\n",
    "        model='embed-multilingual-v3.0',\n",
    "        input_type='search_query'\n",
    "    )\n",
    "    query_embedding = np.array([query_response.embeddings[0]], dtype=np.float32)\n",
    "    \n",
    "    # Search using FAISS\n",
    "    k = min(top_k, len(text_chunks))\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Format the relevant chunks\n",
    "    formatted_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        chunk = text_chunks[idx].strip()\n",
    "        formatted_chunks.append(f\"{chunk}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse détaillée à partir du contexte récupéré en utilisant Gemini.\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    prompt = f\"\"\"Agis comme un expert juridique spécialisé dans l'analyse des lois sur l'enseignement supérieur au Maroc.\n",
    "\n",
    "Contexte du texte de loi:\n",
    "{context[:10000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions précises pour ta réponse:\n",
    "1. Commence par une introduction citant la loi concernée\n",
    "2. Pour chaque mission mentionnée dans la loi:\n",
    "   - Cite le texte exact entre guillemets\n",
    "   - Explique brièvement la portée de cette mission\n",
    "3. Structure ta réponse avec des tirets ou des puces\n",
    "4. Organise les missions par ordre d'importance\n",
    "5. Ajoute une courte conclusion sur l'importance de ces missions\n",
    "\n",
    "FORMAT ATTENDU:\n",
    "- Introduction\n",
    "- Liste détaillée des missions avec citations\n",
    "- Explication pour chaque mission\n",
    "- Conclusion brève\"\"\"\n",
    "\n",
    "    try:\n",
    "        generation_config = {\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.8,\n",
    "            'top_k': 40,\n",
    "            'max_output_tokens': 2048,\n",
    "        }\n",
    "\n",
    "        safety_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'text'):\n",
    "            answer = response.text.strip()\n",
    "        else:\n",
    "            answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "\n",
    "        if len(answer.split()) < 100:\n",
    "            prompt_retry = f\"\"\"Analyse et liste en détail toutes les missions des universités définies dans ce texte de loi:\n",
    "\n",
    "{context[:8000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Inclus des citations exactes du texte et explique chaque mission.\"\"\"\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                prompt_retry,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            if hasattr(response, 'text'):\n",
    "                answer = response.text.strip()\n",
    "            else:\n",
    "                answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "        \n",
    "        if not answer:\n",
    "            return \"Désolé, je n'ai pas pu générer une réponse appropriée. Veuillez reformuler votre question.\"\n",
    "        \n",
    "        if not any(answer.lower().startswith(prefix) for prefix in \n",
    "                  ['selon', 'conformément', 'd\\'après', 'la loi']):\n",
    "            answer = \"Selon la loi 01.00 portant sur l'organisation de l'enseignement supérieur, \" + answer[0].lower() + answer[1:]\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        return f\"\"\"Désolé, une erreur est survenue lors de l'analyse du texte de loi. \n",
    "Suggestions:\n",
    "1. Reformulez votre question de manière plus précise\n",
    "2. Vérifiez que votre question porte bien sur le contenu du texte de loi\n",
    "3. Essayez de poser une question plus spécifique sur une mission particulière\n",
    "\n",
    "Détail technique: {error_msg}\"\"\"\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    index_data = create_faiss_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, index_data)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"quelles sont les mission des universitée ?\"\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        index_data = create_faiss_index.entrypoint(text_chunks)\n",
    "        print(\"FAISS index created successfully\")\n",
    "        \n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            index_data=index_data,\n",
    "            top_k=8\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"\\nRéponse:\")\n",
    "        print(final_answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112351a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:25:10.061744Z",
     "iopub.status.busy": "2024-11-06T15:25:10.060709Z",
     "iopub.status.idle": "2024-11-06T15:25:23.961035Z",
     "shell.execute_reply": "2024-11-06T15:25:23.959908Z",
     "shell.execute_reply.started": "2024-11-06T15:25:10.061700Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f519d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Trying streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174c8b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T13:00:53.963710Z",
     "iopub.status.busy": "2024-11-04T13:00:53.963236Z",
     "iopub.status.idle": "2024-11-04T13:01:04.191194Z",
     "shell.execute_reply": "2024-11-04T13:01:04.189851Z",
     "shell.execute_reply.started": "2024-11-04T13:00:53.963666Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import cohere\n",
    "import google.generativeai as palm\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import os\n",
    "import streamlit as st\n",
    "palm.configure(api_key=\"AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM\")\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec chevauchement optimisé.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=400,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def create_faiss_index(\n",
    "    text_chunks: List[str]\n",
    ") -> Tuple[np.ndarray, List[str], faiss.IndexFlatIP]:\n",
    "    \"\"\"Creates a FAISS index for the text chunks using Cohere embeddings.\"\"\"\n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 96\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch = text_chunks[i:i + batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model='embed-multilingual-v3.0',\n",
    "            input_type='search_document'\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return embeddings, text_chunks, index\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(\n",
    "    question: str,\n",
    "    index_data: Tuple[np.ndarray, List[str], faiss.IndexFlatIP],\n",
    "    top_k: int = 12\n",
    ") -> str:\n",
    "    \"\"\"Retrieves the most relevant chunks using FAISS.\"\"\"\n",
    "    embeddings, text_chunks, index = index_data\n",
    "    \n",
    "    # Initialize Cohere client\n",
    "    co = cohere.Client('A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf')\n",
    "    \n",
    "    # Get embedding for the question\n",
    "    query_response = co.embed(\n",
    "        texts=[question],\n",
    "        model='embed-multilingual-v3.0',\n",
    "        input_type='search_query'\n",
    "    )\n",
    "    query_embedding = np.array([query_response.embeddings[0]], dtype=np.float32)\n",
    "    \n",
    "    # Search using FAISS\n",
    "    k = min(top_k, len(text_chunks))\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Format the relevant chunks\n",
    "    formatted_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        chunk = text_chunks[idx].strip()\n",
    "        formatted_chunks.append(f\"{chunk}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Génère une réponse détaillée à partir du contexte récupéré en utilisant Gemini.\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    prompt = f\"\"\"Agis comme un expert juridique spécialisé dans l'analyse des lois sur l'enseignement supérieur au Maroc.\n",
    "\n",
    "Contexte du texte de loi:\n",
    "{context[:10000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions précises pour ta réponse:\n",
    "1. Commence par une introduction citant la loi concernée\n",
    "2. Pour chaque mission mentionnée dans la loi:\n",
    "   - Cite le texte exact entre guillemets\n",
    "   - Explique brièvement la portée de cette mission\n",
    "3. Structure ta réponse avec des tirets ou des puces\n",
    "4. Organise les missions par ordre d'importance\n",
    "5. Ajoute une courte conclusion sur l'importance de ces missions\n",
    "\n",
    "FORMAT ATTENDU:\n",
    "- Introduction\n",
    "- Liste détaillée des missions avec citations\n",
    "- Explication pour chaque mission\n",
    "- Conclusion brève\"\"\"\n",
    "\n",
    "    try:\n",
    "        generation_config = {\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.8,\n",
    "            'top_k': 40,\n",
    "            'max_output_tokens': 2048,\n",
    "        }\n",
    "\n",
    "        safety_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'text'):\n",
    "            answer = response.text.strip()\n",
    "        else:\n",
    "            answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "\n",
    "        if len(answer.split()) < 100:\n",
    "            prompt_retry = f\"\"\"Analyse et liste en détail toutes les missions des universités définies dans ce texte de loi:\n",
    "\n",
    "{context[:8000]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Inclus des citations exactes du texte et explique chaque mission.\"\"\"\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                prompt_retry,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            if hasattr(response, 'text'):\n",
    "                answer = response.text.strip()\n",
    "            else:\n",
    "                answer = str(response.parts[0].text if hasattr(response, 'parts') else response)\n",
    "        \n",
    "        if not answer:\n",
    "            return \"Désolé, je n'ai pas pu générer une réponse appropriée. Veuillez reformuler votre question.\"\n",
    "        \n",
    "        if not any(answer.lower().startswith(prefix) for prefix in \n",
    "                  ['selon', 'conformément', 'd\\'après', 'la loi']):\n",
    "            answer = \"Selon la loi 01.00 portant sur l'organisation de l'enseignement supérieur, \" + answer[0].lower() + answer[1:]\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        return f\"\"\"Désolé, une erreur est survenue lors de l'analyse du texte de loi. \n",
    "Suggestions:\n",
    "1. Reformulez votre question de manière plus précise\n",
    "2. Vérifiez que votre question porte bien sur le contenu du texte de loi\n",
    "3. Essayez de poser une question plus spécifique sur une mission particulière\n",
    "\n",
    "Détail technique: {error_msg}\"\"\"\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(\n",
    "    pdf_path: str,\n",
    "    question: str\n",
    ") -> str:\n",
    "    \"\"\"Pipeline principal qui orchestre tous les steps.\"\"\"\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    index_data = create_faiss_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, index_data)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"quelles sont les mission des universitée ?\"\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        print(\"PDF text extracted successfully\")\n",
    "        \n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        print(f\"Text preprocessed into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        index_data = create_faiss_index.entrypoint(text_chunks)\n",
    "        print(\"FAISS index created successfully\")\n",
    "        \n",
    "        relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "            question=question,\n",
    "            index_data=index_data,\n",
    "            top_k=8\n",
    "        )\n",
    "        print(\"Retrieved relevant chunks successfully\")\n",
    "        print(\"\\nContext length:\", len(relevant_context))\n",
    "        \n",
    "        final_answer = generate_answer.entrypoint(\n",
    "            question=question,\n",
    "            context=relevant_context\n",
    "        )\n",
    "        \n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"\\nRéponse:\")\n",
    "        print(final_answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97967fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# StreamLIT APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f7e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:25:26.542076Z",
     "iopub.status.busy": "2024-11-06T15:25:26.541230Z",
     "iopub.status.idle": "2024-11-06T15:25:39.883040Z",
     "shell.execute_reply": "2024-11-06T15:25:39.881971Z",
     "shell.execute_reply.started": "2024-11-06T15:25:26.542032Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ccb2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:27:23.368047Z",
     "iopub.status.busy": "2024-11-06T15:27:23.367622Z",
     "iopub.status.idle": "2024-11-06T15:27:23.378760Z",
     "shell.execute_reply": "2024-11-06T15:27:23.377664Z",
     "shell.execute_reply.started": "2024-11-06T15:27:23.368009Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and write to rag_pipeline.py\n",
    "with open('rag_pipeline.py', 'w') as f:\n",
    "    f.write('''from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import cohere\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyB_1802hLH-rIpwx8EAC6uQwcYcxnt0bNM\")\n",
    "\n",
    "@step\n",
    "def preprocess_text(raw_text: str) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=400,\n",
    "        separators=[\"\\\\n\\\\n\",\"\\\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(raw_text)\n",
    "\n",
    "@step\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\\\n\"\n",
    "    return full_text\n",
    "\n",
    "@step\n",
    "def create_faiss_index(text_chunks: List[str]) -> Tuple[np.ndarray, List[str], faiss.IndexFlatIP]:\n",
    "    co = cohere.Client(\"A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf\")\n",
    "    batch_size = 96\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch = text_chunks[i:i + batch_size]\n",
    "        response = co.embed(\n",
    "            texts=batch,\n",
    "            model=\"embed-multilingual-v3.0\",\n",
    "            input_type=\"search_document\"\n",
    "        )\n",
    "        all_embeddings.extend(response.embeddings)\n",
    "    \n",
    "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return embeddings, text_chunks, index\n",
    "\n",
    "@step\n",
    "def retrieve_relevant_chunks(question: str, index_data: Tuple[np.ndarray, List[str], faiss.IndexFlatIP], top_k: int = 12) -> str:\n",
    "    embeddings, text_chunks, index = index_data\n",
    "    co = cohere.Client(\"A1m977Y7aoGcEz1IXgGIeRD7Mcbvq1eHAjQoQ5qf\")\n",
    "    \n",
    "    query_response = co.embed(\n",
    "        texts=[question],\n",
    "        model=\"embed-multilingual-v3.0\",\n",
    "        input_type=\"search_query\"\n",
    "    )\n",
    "    query_embedding = np.array([query_response.embeddings[0]], dtype=np.float32)\n",
    "    \n",
    "    k = max(min(top_k, len(text_chunks)), 5)\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    formatted_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        if idx < len(text_chunks):\n",
    "            chunk = text_chunks[idx].strip()\n",
    "            if chunk:\n",
    "                formatted_chunks.append(chunk)\n",
    "    \n",
    "    if not formatted_chunks:\n",
    "        return \"Aucun contexte pertinent trouvé.\"\n",
    "    \n",
    "    return \"\\\\n\\\\n\".join(formatted_chunks)\n",
    "\n",
    "@step\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Tu es un assistant juridique expert spécialisé dans l'analyse des lois sur l'enseignement supérieur au Maroc. \"\n",
    "        f\"Réponds de manière naturelle et appropriée à la question posée.\\\\n\\\\n\"\n",
    "        f\"Contexte du texte de loi:\\\\n{context[:10000]}\\\\n\\\\n\"\n",
    "        f\"Question de l'utilisateur: {question}\\\\n\\\\n\"\n",
    "        f\"Instructions pour ta réponse:\\\\n\"\n",
    "        f\"1. Si la question est une salutation ou une question générale, réponds de manière appropriée et courtoise\\\\n\"\n",
    "        f\"2. Si la question porte sur le contenu de la loi:\\\\n\"\n",
    "        f\"   - Cite les passages pertinents entre guillemets\\\\n\"\n",
    "        f\"   - Explique clairement leur signification\\\\n\"\n",
    "        f\"   - Structure ta réponse de manière logique\\\\n\"\n",
    "        f\"3. Adapte ton niveau de détail à la question posée\\\\n\"\n",
    "        f\"4. Reste factuel et précis\\\\n\"\n",
    "        f\"5. Si la question n'est pas liée au contexte juridique, réponds poliment sans citer la loi\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40,\n",
    "            \"max_output_tokens\": 2048,\n",
    "        }\n",
    "\n",
    "        safety_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        \n",
    "        answer = \"\"\n",
    "        if response.parts:\n",
    "            answer = response.parts[0].text\n",
    "        else:\n",
    "            retry_prompt = (\n",
    "                f\"Analyse et explique ce point spécifique de la loi 01.00:\\\\n\\\\n\"\n",
    "                f\"Contexte:\\\\n{context[:8000]}\\\\n\\\\n\"\n",
    "                f\"Question: {question}\\\\n\\\\n\"\n",
    "                f\"Instructions:\\\\n\"\n",
    "                f\"- Cite les articles pertinents\\\\n\"\n",
    "                f\"- Explique leur signification\\\\n\"\n",
    "                f\"- Structure ta réponse clairement\"\n",
    "            )\n",
    "            \n",
    "            retry_response = model.generate_content(\n",
    "                retry_prompt,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            if retry_response.parts:\n",
    "                answer = retry_response.parts[0].text\n",
    "            else:\n",
    "                return \"Je m'excuse, je n'ai pas pu générer une réponse appropriée. Pourriez-vous reformuler votre question ?\"\n",
    "\n",
    "        answer = answer.strip()\n",
    "        if not answer:\n",
    "            return \"Je m'excuse, je n'ai pas pu générer une réponse appropriée. Pourriez-vous reformuler votre question ?\"\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Debug - Error details: {str(e)}\")\n",
    "        return \"Je m'excuse, je n'ai pas pu accéder aux informations demandées. Pourriez-vous reformuler votre question ?\"\n",
    "\n",
    "@pipeline\n",
    "def rag_pipeline(pdf_path: str, question: str) -> str:\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    text_chunks = preprocess_text(raw_text)\n",
    "    index_data = create_faiss_index(text_chunks)\n",
    "    relevant_context = retrieve_relevant_chunks(question, index_data)\n",
    "    answer = generate_answer(question, relevant_context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"_main_\":\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    question = \"quelles sont les mission des universitée ?\"\n",
    "    try:\n",
    "        pipeline = rag_pipeline(pdf_path, question)\n",
    "        print(pipeline)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        \n",
    "   \n",
    "                \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc195e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:27:33.703273Z",
     "iopub.status.busy": "2024-11-06T15:27:33.702847Z",
     "iopub.status.idle": "2024-11-06T15:27:34.789211Z",
     "shell.execute_reply": "2024-11-06T15:27:34.788077Z",
     "shell.execute_reply.started": "2024-11-06T15:27:33.703232Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5180d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:27:44.437750Z",
     "iopub.status.busy": "2024-11-06T15:27:44.437321Z",
     "iopub.status.idle": "2024-11-06T15:45:04.840295Z",
     "shell.execute_reply": "2024-11-06T15:45:04.839160Z",
     "shell.execute_reply.started": "2024-11-06T15:27:44.437708Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!npm install -y localtunnel\n",
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60abab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T15:27:28.593147Z",
     "iopub.status.busy": "2024-11-06T15:27:28.592713Z",
     "iopub.status.idle": "2024-11-06T15:27:28.601745Z",
     "shell.execute_reply": "2024-11-06T15:27:28.600752Z",
     "shell.execute_reply.started": "2024-11-06T15:27:28.593079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and write to app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write('''\n",
    "# Streamlit App for Legal Assistant\n",
    "import streamlit as st\n",
    "from zenml import step, pipeline\n",
    "import PyPDF2\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from rag_pipeline import (\n",
    "    extract_pdf_text,\n",
    "    preprocess_text,\n",
    "    create_faiss_index,\n",
    "    retrieve_relevant_chunks,\n",
    "    generate_answer\n",
    ")\n",
    "\n",
    "# Configure page settings\n",
    "st.set_page_config(\n",
    "    page_title=\"Assistant Juridique - Loi sur l'Enseignement Supérieur\",\n",
    "    page_icon=\"📚\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Add custom CSS\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .chat-container {\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f3ff;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Initialize session state\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if 'index_data' not in st.session_state:\n",
    "    st.session_state.index_data = None\n",
    "\n",
    "def initialize_rag_system(pdf_path: str):\n",
    "    \"\"\"Initialize the RAG system with the PDF document.\"\"\"\n",
    "    try:\n",
    "        raw_text = extract_pdf_text.entrypoint(pdf_path)\n",
    "        text_chunks = preprocess_text.entrypoint(raw_text)\n",
    "        index_data = create_faiss_index.entrypoint(text_chunks)\n",
    "        st.session_state.index_data = index_data\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors de l'initialisation: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.title(\"🤖 Assistant Juridique\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### À propos\n",
    "    Cet assistant vous aide à comprendre la loi 01.00 sur l'organisation de l'enseignement supérieur au Maroc.\n",
    "    \n",
    "    ### Comment utiliser\n",
    "    1. Posez votre question sur la loi\n",
    "    2. L'assistant analysera le texte de loi\n",
    "    3. Vous recevrez une réponse\n",
    "    \"\"\")\n",
    "    \n",
    "    # Add PDF upload option\n",
    "    uploaded_file = st.file_uploader(\"Charger un nouveau PDF de loi\", type=\"pdf\")\n",
    "    if uploaded_file:\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(uploaded_file.getvalue())\n",
    "        if initialize_rag_system(\"temp.pdf\"):\n",
    "            st.success(\"PDF chargé et indexé avec succès!\")\n",
    "\n",
    "# Main chat interface\n",
    "st.title(\"💬 Assistant Juridique - Loi sur l'Enseignement Supérieur\")\n",
    "\n",
    "# Initialize system with default PDF if not already done\n",
    "if st.session_state.index_data is None:\n",
    "    pdf_path = \"/kaggle/input/loi-n-01/loi-n-01-00-portant-organisation-de-lenseignement-suprieur.pdf\"\n",
    "    if os.path.exists(pdf_path):\n",
    "        initialize_rag_system(pdf_path)\n",
    "    else:\n",
    "        st.warning(\"Veuillez charger un fichier PDF de la loi pour commencer.\")\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.container():\n",
    "        if message[\"role\"] == \"user\":\n",
    "            st.markdown(f'<div class=\"user-message\">👤 Vous : {message[\"content\"]}</div>', \n",
    "                       unsafe_allow_html=True)\n",
    "        else:\n",
    "            st.markdown(f'<div class=\"assistant-message\">🤖 Assistant : {message[\"content\"]}</div>', \n",
    "                       unsafe_allow_html=True)\n",
    "\n",
    "# Chat input\n",
    "if question := st.chat_input(\"Posez votre question sur la loi...\"):\n",
    "    # Add user message to chat\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    if st.session_state.index_data is not None:\n",
    "        try:\n",
    "            with st.spinner(\"Recherche en cours...\"):\n",
    "                # Get relevant context\n",
    "                relevant_context = retrieve_relevant_chunks.entrypoint(\n",
    "                    question=question,\n",
    "                    index_data=st.session_state.index_data,\n",
    "                    top_k=8\n",
    "                )\n",
    "                \n",
    "                # Generate answer\n",
    "                answer = generate_answer.entrypoint(\n",
    "                    question=question,\n",
    "                    context=relevant_context\n",
    "                )\n",
    "                \n",
    "                # Add assistant message to chat\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "                \n",
    "                # Rerun to update the chat display\n",
    "                st.rerun()\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Erreur lors du traitement de votre question: {str(e)}\")\n",
    "    else:\n",
    "        st.warning(\"Le système n'est pas encore initialisé. Veuillez charger un fichier PDF.\")\n",
    "\n",
    "# Add a clear chat button\n",
    "if st.button(\"Effacer la conversation\"):\n",
    "    st.session_state.messages = []\n",
    "    st.rerun()\n",
    "# Rest of your code...\n",
    "# [Include all the remaining code from your original file]\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5997199,
     "sourceId": 9787834,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 170.570166,
   "end_time": "2024-11-06T16:08:34.889116",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-06T16:05:44.318950",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
